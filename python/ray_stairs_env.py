import gymnasium as gym
from gymnasium import spaces
import numpy as np
import mujoco
import os


class RayFocusedStairs(gym.Env):
    """
    Overnight Training Environment for Stair Climbing & Descending.

    Task: Traverse the 'Scenario 3' Pyramid (Up and Down).
    Action: 1D Control (Bogie Angle).
    Drive: Constant 0.4 m/s.
    """

    def __init__(self, render_mode=None):
        super(RayFocusedStairs, self).__init__()

        # 1. SETUP MUJOCO
        # Locate the XML generated by build_ray.py (Scenario 3)
        script_dir = os.path.dirname(os.path.abspath(__file__))
        xml_path = os.path.join(script_dir, "..", "mujoco", "ray_simulation.xml")
        xml_path = os.path.normpath(xml_path)

        if not os.path.exists(xml_path):
            raise FileNotFoundError("XML not found! Run 'build_ray.py' (Select Scenario 3) first.")

        self.model = mujoco.MjModel.from_xml_path(xml_path)
        self.data = mujoco.MjData(self.model)

        # Get Actuator IDs
        self.drive_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_ACTUATOR, "drive_forward")
        self.climb_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_ACTUATOR, "actuator_climb")
        self.bin_id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_ACTUATOR, "level_bin")

        # 2. ACTION SPACE
        # Control: Bogie Angle [-1.5 to 1.5 rad]
        self.action_space = spaces.Box(low=np.array([-1.5]), high=np.array([1.5]), dtype=np.float32)

        # 3. OBSERVATION SPACE
        # [Pitch, Roll, Bogie_Pos, Floor_L, Floor_U, Wall, Velocity]
        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(7,), dtype=np.float32)

        self.render_mode = render_mode
        self.viewer = None

        # Internal State for Smoothing
        self.filtered_action = 0.0
        self.last_raw_action = 0.0

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        mujoco.mj_resetData(self.model, self.data)

        # 4. RANDOMIZED SPAWN (The Expert Strategy)
        # 50% Start at Bottom (Learn to Climb)
        # 50% Start at Top (Learn to Descend)
        rand = np.random.random()

        if rand < 0.5:
            # CLIMB MODE
            self.data.qpos[0] = 1.0  # Run-up to stairs (at X=2.0)
            self.data.qpos[2] = 0.2  # Floor height
        else:
            # DESCEND MODE
            self.data.qpos[0] = 3.5  # Top of platform
            self.data.qpos[2] = 0.6  # Platform height (~0.5m)

        # Add slight lateral noise to prevent overfitting
        self.data.qpos[1] = 0.0 + np.random.uniform(-0.1, 0.1)

        # Heading: Face +X direction (Quat [1, 0, 0, 0])
        self.data.qpos[3:7] = [1, 0, 0, 0]

        # Reset Memories
        self.filtered_action = 0.0
        self.last_raw_action = 0.0

        # Settle Physics
        mujoco.mj_step(self.model, self.data)

        return self._get_obs(), {}

    def step(self, action):
        raw_action = action[0]

        # 5. ACTION SMOOTHING (Low Pass Filter)
        # Prevents "Spazzing" / Vibration
        alpha = 0.2  # 20% blend factor
        self.filtered_action = (alpha * raw_action) + ((1 - alpha) * self.filtered_action)

        # Apply Controls
        self.data.ctrl[self.climb_id] = self.filtered_action
        self.data.ctrl[self.drive_id] = 0.4  # Constant slow drive
        self.data.ctrl[self.bin_id] = -self._get_pitch()  # Auto-Balance Bin

        # 6. PHYSICS STEP
        # Run 20 physics ticks per 1 AI tick
        for _ in range(20):
            mujoco.mj_step(self.model, self.data)

        # 7. REWARD CALCULATION

        # A. Progress Reward (CAPPED)
        # We cap velocity at 0.5 so falling off a cliff isn't rewarded.
        vel_x = self.data.qvel[0]
        safe_vel = np.clip(vel_x, -0.1, 0.5)
        reward_progress = safe_vel * 5.0

        # B. Stability Penalty
        # Critical for descent: punish pitching forward
        reward_stability = -abs(self._get_pitch()) * 2.0

        # C. Efficiency & Smoothness
        # Punish holding legs up unnecessarily
        reward_energy = -np.square(raw_action) * 0.2
        # Punish twitchy movement
        reward_smooth = -np.square(raw_action - self.last_raw_action) * 5.0

        # TOTAL
        total_reward = reward_progress + reward_stability + reward_energy + reward_smooth
        self.last_raw_action = raw_action

        # 8. TERMINATION
        terminated = False

        # Fail: Flipped Over
        if abs(self._get_pitch()) > 1.0 or abs(self._get_roll()) > 1.0:
            terminated = True
            total_reward -= 50.0  # Crash penalty

        # Success: Cleared the Pyramid (X > 6.0)
        # Note: Pyramid ends at X=5.0, so 6.0 confirms full clearance
        if self.data.qpos[0] > 6.0:
            terminated = True
            total_reward += 1000.0  # Mission Complete

        if self.render_mode == "human":
            self._render_frame()

        return self._get_obs(), total_reward, terminated, False, {}

    # --- HELPERS ---
    def _get_obs(self):
        pitch = self._get_pitch()
        roll = self._get_roll()
        bogie = self.data.ctrl[self.climb_id]
        vel = np.linalg.norm(self.data.qvel[:2])

        # Read the 3 Rangefinders
        l1 = self._read_sensor("floor_sensL")
        l2 = self._read_sensor("floor_sensU")
        l3 = self._read_sensor("wall_sens")

        return np.array([pitch, roll, bogie, l1, l2, l3, vel], dtype=np.float32)

    def _get_pitch(self):
        # Convert Body Quaternion to Pitch
        q = self.data.qpos[3:7]
        return np.arcsin(np.clip(2 * (q[0] * q[2] - q[3] * q[1]), -1, 1))

    def _get_roll(self):
        # Convert Body Quaternion to Roll
        q = self.data.qpos[3:7]
        return np.arctan2(2 * (q[0] * q[1] + q[2] * q[3]), 1 - 2 * (q[1] ** 2 + q[2] ** 2))

    def _read_sensor(self, name):
        try:
            id = mujoco.mj_name2id(self.model, mujoco.mjtObj.mjOBJ_SENSOR, name)
            adr = self.model.sensor_adr[id]
            val = self.data.sensordata[adr]
            # If sensor returns -1 (no hit), treat as 2.0 meters (safe distance)
            return 2.0 if val < 0 else val
        except:
            return 0.0

    def _render_frame(self):
        if self.viewer is None:
            import mujoco.viewer
            self.viewer = mujoco.viewer.launch_passive(self.model, self.data)
        self.viewer.sync()

    def close(self):
        if self.viewer: self.viewer.close()